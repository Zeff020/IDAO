{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import utils\n",
    "import scoring\n",
    "import numpy as np\n",
    "\n",
    "from skopt import Optimizer\n",
    "\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import Matern, RBF, WhiteKernel\n",
    "\n",
    "from skopt.learning import RandomForestRegressor\n",
    "\n",
    "from skopt.acquisition import gaussian_ei as acq_func\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing and structuring data ###\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "train, test = utils.load_small_data_csv(DATA_PATH,\"train_smaller100.csv.gz\" , \"test_smaller100.csv.gz\", utils.SIMPLE_FEATURE_COLUMNS)\n",
    "\n",
    "PointResiduals,Angles = utils.kink(train)\n",
    "train['PointResiduals'] = pd.Series(PointResiduals, index=train.index)\n",
    "train['Angles'] = pd.Series(PointResiduals, index=train.index)\n",
    "\n",
    "\n",
    "train_part, val_part = train_test_split(train, test_size=0.20, shuffle=True)\n",
    "x_train = train_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values\n",
    "x_val   =  val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values\n",
    "y_train = train_part.loc[:, [\"label\"]].values\n",
    "y_val = val_part.loc[:, [\"label\"]].values\n",
    "#y_train_weight = train_part.loc[:, [\"weight\"]].values\n",
    "\n",
    "# turn labels into categorical classes\n",
    "classes = [0,1]\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=len(classes))\n",
    "y_val   = keras.utils.to_categorical(y_val,   num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining utils ###\n",
    "\n",
    "# Rectified linear unit\n",
    "\n",
    "def relu(x):\n",
    "  return np.array([ (i>0) * abs(i) for i in x ])\n",
    "\n",
    "# plotting the bayesian optimizer\n",
    "\n",
    "def plot_bo(bo, suggestion=None, value=None):\n",
    "    a, b = bo.space.bounds[0]\n",
    "    \n",
    "    # getting the latest model\n",
    "    model = bo.models[-1]\n",
    "    \n",
    "    xs = np.linspace(a, b, num=100)\n",
    "    x_model = bo.space.transform(xs.reshape(-1, 1).tolist())\n",
    "    \n",
    "    mean, std = model.predict(x_model, return_std=True)\n",
    "    \n",
    "    plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(\n",
    "        np.array(bo.Xi)[:, 0],\n",
    "        np.array(bo.yi),\n",
    "        color='red',\n",
    "        label='observations'\n",
    "    )\n",
    "    if suggestion is not None:\n",
    "        plt.scatter([suggestion], value, color='blue', label='suggestion')\n",
    "    \n",
    "    plt.plot(xs, mean, color='green', label='model')\n",
    "    plt.fill_between(xs, mean - 1.96 * std, mean + 1.96 * std, alpha=0.1, color='green')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    acq = acq_func(x_model, model, np.min(bo.yi))\n",
    "    plt.plot(xs, acq, label='Expected Improvement')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "\n",
    "def cum_min(xs):\n",
    "    result = np.zeros_like(xs)\n",
    "    cmin = xs[0]\n",
    "    \n",
    "    result[0] = xs[0]\n",
    "    \n",
    "    for i in range(1, xs.shape[0]):\n",
    "        if cmin > xs[i]:\n",
    "            cmin = xs[i]\n",
    "\n",
    "        result[i] = cmin\n",
    "    \n",
    "    return result\n",
    "\n",
    "# plots progress of BO over time\n",
    "\n",
    "def plot_convergence(bo):\n",
    "    display.clear_output(wait=True)\n",
    "    values = np.array(bo.yi)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cum_min(values), label='minimal discovered')\n",
    "    plt.scatter(np.arange(len(bo.yi)), bo.yi, label='observations')\n",
    "    plt.xlabel('step', fontsize=14)\n",
    "    plt.ylabel('loss', fontsize=14)\n",
    "    \n",
    "    plt.legend(loc='upper right', fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Prints best parameters\n",
    "    \n",
    "def print_best(bo):\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    NodesInFirstDense, NodesInSecondDense, DropoutValue, INIT_LEARNINGRATE = best_parameters\n",
    "    \n",
    "    print(\n",
    "        'Best model:\\n Nodes in first dense layer= {0} \\n Nodes in second dense layer= {1} \\n learning rate= {2} \\n Dropout value= {3}'.format(\n",
    "            int(np.ceil(NodesInFirstDense)),\n",
    "            int(np.ceil(NodesInSecondDense)),\n",
    "            INIT_LEARNINGRATE,\n",
    "            DropoutValue\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Target function with as input optimizeable parameters ###\n",
    "\n",
    "def target_function1(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense( len(classes) )) # muon and 'other'\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function2(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, NodesInThirdDense, INIT_LEARNINGRATE, DropoutValue  = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    NodesInThirdDense = int(np.ceil(NodesInThirdDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense(NodesInThirdDense )) # muon and 'other'\n",
    "    model.add(Dense( len(classes) )) # muon and 'other'\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function3(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, NodesInThirdDense, NodesInFourthDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    NodesInThirdDense = int(np.ceil(NodesInThirdDense))\n",
    "    NodesInFourthDense = int(np.ceil(NodesInFourthDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense(NodesInThirdDense )) # muon and 'other'\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInFourthDense )) # muon and 'other'\n",
    "    model.add(Dense( len(classes) ))\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function4(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, NodesInThirdDense, NodesInFourthDense,NodesInFifthDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    NodesInThirdDense = int(np.ceil(NodesInThirdDense))\n",
    "    NodesInFourthDense = int(np.ceil(NodesInFourthDense))\n",
    "    NodesInFifthDense = int(np.ceil(NodesInFifthDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense(NodesInThirdDense )) # muon and 'other'\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInFourthDense )) # muon and 'other'\n",
    "    model.add(Dense(NodesInFifthDense ))\n",
    "    model.add(Dense( len(classes) ))\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_NoBatch(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense( len(classes) ))\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting dimensions for optimizeable parameters ###\n",
    "\n",
    "dimensions_4 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]\n",
    "\n",
    "dimensions_5 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInThirdDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]\n",
    "\n",
    "dimensions_6 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInThirdDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInFourthDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]\n",
    "\n",
    "dimensions_7 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInThirdDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInFourthDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInFifthDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random forest regressor optimizer ###\n",
    "\n",
    "bo_rf_41 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_42 = Optimizer(\n",
    "    dimensions=dimensions_5,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_43 = Optimizer(\n",
    "    dimensions=dimensions_6,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_44 = Optimizer(\n",
    "    dimensions=dimensions_7,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110.05943498004768, 249.83726673492947, 278.3774397943327, 10.88517063143468, 230.63392702049586, 0.009118547776782965, 0.06663418011144207]\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 16s 364us/step - loss: 0.2949 - acc: 0.9228 - val_loss: 0.2413 - val_acc: 0.9270\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 14s 313us/step - loss: 0.2439 - acc: 0.9243 - val_loss: 0.2378 - val_acc: 0.9279\n",
      "Epoch 3/3\n",
      "36672/43565 [========================>.....] - ETA: 2s - loss: 0.2386 - acc: 0.9246"
     ]
    }
   ],
   "source": [
    "OptimalParams1 = ([])\n",
    "OptimalParams2 = ([])\n",
    "OptimalParams3= ([])\n",
    "OptimalParams4= ([])\n",
    "OptimalParams_NoBatch = ([])\n",
    "\n",
    "OptimalScore1 = ([])\n",
    "OptimalScore2 = ([])\n",
    "OptimalScore3= ([])\n",
    "OptimalScore4= ([])\n",
    "OptimalScore_NoBatch = ([])\n",
    "\n",
    "for i in range(0,3):\n",
    "\n",
    "    bo_rf_41 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_41\n",
    "\n",
    "    for j in range(100):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function1(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams1 = np.append(OptimalParams1,best_parameters, axis=0)\n",
    "    OptimalScore1 = np.append(OptimalScore1,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams1 = np.append(OptimalParams1,best_parameters, axis=0)\n",
    "    OptimalScore1 = np.append(OptimalScore1,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams1 = np.append(OptimalParams1,best_parameters, axis=0)\n",
    "    OptimalScore1 = np.append(OptimalScore1,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Second network, three dense layers\n",
    "for i in range(0,3):\n",
    "\n",
    "    bo_rf_42 = Optimizer(\n",
    "    dimensions=dimensions_5,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_42\n",
    "\n",
    "    for j in range(150):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function2(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "\n",
    "    OptimalParams2 = np.append(OptimalParams2, best_parameters, axis=0)\n",
    "    OptimalScore2 = np.append(OptimalScore2,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams2 = np.append(OptimalParams2,best_parameters, axis=0)\n",
    "    OptimalScore2 = np.append(OptimalScore2,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams2 = np.append(OptimalParams2,best_parameters, axis=0)\n",
    "    OptimalScore2 = np.append(OptimalScore2,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "#Third network, four dense layers\n",
    "for i in range(0,3):\n",
    "\n",
    "    bo_rf_43 = Optimizer(\n",
    "    dimensions=dimensions_6,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_43\n",
    "\n",
    "    for j in range(180):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function3(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams3 = np.append(OptimalParams3, best_parameters, axis=0)\n",
    "    OptimalScore3 = np.append(OptimalScore3,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams3 = np.append(OptimalParams3,best_parameters, axis=0)\n",
    "    OptimalScore3 = np.append(OptimalScore3,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams3 = np.append(OptimalParams3,best_parameters, axis=0)\n",
    "    OptimalScore3 = np.append(OptimalScore3,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# fourth network, five dense layers\n",
    "for i in range(0,3):\n",
    "\n",
    "    bo_rf_44 = Optimizer(\n",
    "    dimensions=dimensions_7,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_44\n",
    "\n",
    "    for j in range(200):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function4(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams4 = np.append(OptimalParams4, best_parameters, axis = 0)\n",
    "    OptimalScore4 = np.append(OptimalScore4,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams4 = np.append(OptimalParams4,best_parameters, axis=0)\n",
    "    OptimalScore4 = np.append(OptimalScore4,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams4 = np.append(OptimalParams4,best_parameters, axis=0)\n",
    "    OptimalScore4 = np.append(OptimalScore4,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# fifth network, No batch normalization\n",
    "for i in range(0,3):        \n",
    "\n",
    "    bo_rf_41 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_41\n",
    "\n",
    "    for j in range(100):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function_NoBatch(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams_NoBatch = np.append(OptimalParams_NoBatch, best_parameters, axis=0)\n",
    "    OptimalScore_NoBatch = np.append(OptimalScore_NoBatch,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams_NoBatch = np.append(OptimalParams_NoBatch, best_parameters, axis=0)\n",
    "    OptimalScore_NoBatch = np.append(OptimalScore_NoBatch,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams_NoBatch = np.append(OptimalParams_NoBatch,best_parameters, axis=0)\n",
    "    OptimalScore_NoBatch = np.append(OptimalScore_NoBatch,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63773383, 0.64893887, 1.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OptimalScore3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_index = np.argmin(bo.yi)\n",
    "best_parameters = bo.Xi[best_result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5384148328829712"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.yi[np.argmin(bo.yi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.yi[np.argmin(bo.yi)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5414268371482693"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.yi[np.argmin(bo.yi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[173.2773707552993,\n",
       " 14.45812940532587,\n",
       " 0.007589349274725483,\n",
       " 0.008446173333937724]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result_index = np.argmin(bo.yi)\n",
    "best_parameters = bo.Xi[best_result_index]\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53841483])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(OptimalScore1,bo.yi[np.argmin(bo.yi)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5900488114779627"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result_index = np.argmin(bo.yi)\n",
    "best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "best_result_index\n",
    "best_parameters\n",
    "bo.yi[np.argmin(bo.yi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gaussian optimizer ###\n",
    "\n",
    "bo_gp_1 = Optimizer(\n",
    "    ### telling optimizer boundaries for each parameter\n",
    "    dimensions=dimensions_1,\n",
    "    \n",
    "    ### setting regressor\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-6, 1.0e+6]) + \\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2]),\n",
    "    ),\n",
    "    n_initial_points=2,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Actual optimization process ###\n",
    "def Optimization(bo,target_function):\n",
    "    Parameter_List = ([])\n",
    "    for i in range(1):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function(x) # Other inputs are automatically set\n",
    "\n",
    "        Parameter_List = np.append(Parameter_List, x)\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "        \n",
    "        best_result_index = np.argmin(bo.yi)\n",
    "        best_parameters = bo.Xi[best_result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:\n",
      " Nodes in first dense layer= 235 \n",
      " Nodes in second dense layer= 147 \n",
      " learning rate= 0.02177968898175681 \n",
      " Dropout value= 0.005580370632707155\n"
     ]
    }
   ],
   "source": [
    "print_best(bo_rf_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 271)               17886     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 271)               1084      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 271)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 280)               76160     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 562       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 95,692\n",
      "Trainable params: 95,150\n",
      "Non-trainable params: 542\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 14s 321us/step - loss: 0.2786 - acc: 0.9236 - val_loss: 0.2719 - val_acc: 0.9250\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 265us/step - loss: 0.2433 - acc: 0.9250 - val_loss: 0.2521 - val_acc: 0.9247\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 249us/step - loss: 0.2363 - acc: 0.9251 - val_loss: 0.2567 - val_acc: 0.9266\n",
      "0.3481612531447594\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 260us/step - loss: 0.2347 - acc: 0.9254 - val_loss: 0.2492 - val_acc: 0.9261\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 266us/step - loss: 0.2334 - acc: 0.9253 - val_loss: 0.2366 - val_acc: 0.9262\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 243us/step - loss: 0.2326 - acc: 0.9258 - val_loss: 0.2347 - val_acc: 0.9266\n",
      "0.3777592072609885\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 246us/step - loss: 0.2322 - acc: 0.9259 - val_loss: 0.2292 - val_acc: 0.9276\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 278us/step - loss: 0.2303 - acc: 0.9265 - val_loss: 0.2333 - val_acc: 0.9264\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 245us/step - loss: 0.2311 - acc: 0.9263 - val_loss: 0.2338 - val_acc: 0.9273\n",
      "0.3755042846797935\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 261us/step - loss: 0.2310 - acc: 0.9257 - val_loss: 0.2362 - val_acc: 0.9255\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 268us/step - loss: 0.2297 - acc: 0.9270 - val_loss: 0.2340 - val_acc: 0.9248\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 248us/step - loss: 0.2309 - acc: 0.9260 - val_loss: 0.2338 - val_acc: 0.9249\n",
      "0.4127228370619375\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 262us/step - loss: 0.2295 - acc: 0.9256 - val_loss: 0.2429 - val_acc: 0.9262\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 11s 253us/step - loss: 0.2300 - acc: 0.9252 - val_loss: 0.2349 - val_acc: 0.9266\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 12s 268us/step - loss: 0.2312 - acc: 0.9258 - val_loss: 0.2394 - val_acc: 0.9265\n",
      "0.3827155505030586\n"
     ]
    }
   ],
   "source": [
    "### Testing the found parameters to see if the results are reproduceable ###\n",
    "\n",
    "INIT_LEARNINGRATE = 0.02532767329545395  \n",
    "BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "EPOCHS = 3\n",
    "\n",
    "assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "#assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(271, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.0009682674694188368))\n",
    "model.add(Dense(280, activation='relu'))\n",
    "model.add(Dense( len(classes) )) # muon and 'other'\n",
    "model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "    metrics=['accuracy'] \n",
    "    )\n",
    "\n",
    "# Running test for 5 times in this case\n",
    "for i in range(0,5):\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               6600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 12,152\n",
      "Trainable params: 11,952\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 8s 176us/step - loss: 0.2471 - acc: 0.9252 - val_loss: 0.2437 - val_acc: 0.9234\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 6s 143us/step - loss: 0.2357 - acc: 0.9267 - val_loss: 0.2456 - val_acc: 0.9224\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 6s 141us/step - loss: 0.2323 - acc: 0.9263 - val_loss: 0.2408 - val_acc: 0.9236\n",
      "0.3487180061696864\n"
     ]
    }
   ],
   "source": [
    "#### Original keras model for reference ####\n",
    "\n",
    "INIT_LEARNINGRATE = 5e-3\n",
    "BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "EPOCHS = 3\n",
    "\n",
    "assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "#assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50 , activation='relu'))\n",
    "model.add(Dense( len(classes) )) # muon and 'other'\n",
    "model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "    metrics=['accuracy'] \n",
    "    )\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (x_val, y_val),\n",
    "    shuffle = True\n",
    "    )\n",
    "\n",
    "#model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "# score\n",
    "\n",
    "validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    \n",
    "    for j in range(0,5):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nproc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (65,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"c:\\miniconda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 952, in fit\n    batch_size=batch_size)\n  File \"c:\\miniconda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 751, in _standardize_user_data\n    exception_prefix='input')\n  File \"c:\\miniconda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 138, in standardize_input_data\n    str(data_shape))\nValueError: Error when checking input: expected dense_1_input to have shape (65,) but got array with shape (2,)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f031f348a31a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         '''\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (65,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "r = pool.map(model.fit, [x_train, y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
