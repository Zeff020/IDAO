{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import utils\n",
    "import scoring\n",
    "import numpy as np\n",
    "\n",
    "from skopt import Optimizer\n",
    "\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import Matern, RBF, WhiteKernel\n",
    "\n",
    "from skopt.learning import RandomForestRegressor\n",
    "\n",
    "from skopt.acquisition import gaussian_ei as acq_func\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing and structuring data ###\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "train, test = utils.load_small_data_csv(DATA_PATH,\"train_smaller100.csv.gz\" , \"test_smaller100.csv.gz\", utils.SIMPLE_FEATURE_COLUMNS)\n",
    "\n",
    "PointResiduals,Angles = utils.kink(train)\n",
    "train['PointResiduals'] = pd.Series(PointResiduals, index=train.index)\n",
    "train['Angles'] = pd.Series(PointResiduals, index=train.index)\n",
    "\n",
    "\n",
    "train_part, val_part = train_test_split(train, test_size=0.20, shuffle=True)\n",
    "x_train = train_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values\n",
    "x_val   =  val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values\n",
    "y_train = train_part.loc[:, [\"label\"]].values\n",
    "y_val = val_part.loc[:, [\"label\"]].values\n",
    "#y_train_weight = train_part.loc[:, [\"weight\"]].values\n",
    "\n",
    "# turn labels into categorical classes\n",
    "classes = [0,1]\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=len(classes))\n",
    "y_val   = keras.utils.to_categorical(y_val,   num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining utils ###\n",
    "\n",
    "# Rectified linear unit\n",
    "\n",
    "def relu(x):\n",
    "  return np.array([ (i>0) * abs(i) for i in x ])\n",
    "\n",
    "# plotting the bayesian optimizer\n",
    "\n",
    "def plot_bo(bo, suggestion=None, value=None):\n",
    "    a, b = bo.space.bounds[0]\n",
    "    \n",
    "    # getting the latest model\n",
    "    model = bo.models[-1]\n",
    "    \n",
    "    xs = np.linspace(a, b, num=100)\n",
    "    x_model = bo.space.transform(xs.reshape(-1, 1).tolist())\n",
    "    \n",
    "    mean, std = model.predict(x_model, return_std=True)\n",
    "    \n",
    "    plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(\n",
    "        np.array(bo.Xi)[:, 0],\n",
    "        np.array(bo.yi),\n",
    "        color='red',\n",
    "        label='observations'\n",
    "    )\n",
    "    if suggestion is not None:\n",
    "        plt.scatter([suggestion], value, color='blue', label='suggestion')\n",
    "    \n",
    "    plt.plot(xs, mean, color='green', label='model')\n",
    "    plt.fill_between(xs, mean - 1.96 * std, mean + 1.96 * std, alpha=0.1, color='green')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    acq = acq_func(x_model, model, np.min(bo.yi))\n",
    "    plt.plot(xs, acq, label='Expected Improvement')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "\n",
    "def cum_min(xs):\n",
    "    result = np.zeros_like(xs)\n",
    "    cmin = xs[0]\n",
    "    \n",
    "    result[0] = xs[0]\n",
    "    \n",
    "    for i in range(1, xs.shape[0]):\n",
    "        if cmin > xs[i]:\n",
    "            cmin = xs[i]\n",
    "\n",
    "        result[i] = cmin\n",
    "    \n",
    "    return result\n",
    "\n",
    "# plots progress of BO over time\n",
    "\n",
    "def plot_convergence(bo):\n",
    "    display.clear_output(wait=True)\n",
    "    values = np.array(bo.yi)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cum_min(values), label='minimal discovered')\n",
    "    plt.scatter(np.arange(len(bo.yi)), bo.yi, label='observations')\n",
    "    plt.xlabel('step', fontsize=14)\n",
    "    plt.ylabel('loss', fontsize=14)\n",
    "    \n",
    "    plt.legend(loc='upper right', fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Prints best parameters\n",
    "    \n",
    "def print_best(bo):\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    NodesInFirstDense, NodesInSecondDense, DropoutValue, INIT_LEARNINGRATE = best_parameters\n",
    "    \n",
    "    print(\n",
    "        'Best model:\\n Nodes in first dense layer= {0} \\n Nodes in second dense layer= {1} \\n learning rate= {2} \\n Dropout value= {3}'.format(\n",
    "            int(np.ceil(NodesInFirstDense)),\n",
    "            int(np.ceil(NodesInSecondDense)),\n",
    "            INIT_LEARNINGRATE,\n",
    "            DropoutValue\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Target function with as input optimizeable parameters ###\n",
    "\n",
    "def target_function1(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense( len(classes) )) # muon and 'other'\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function2(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, NodesInThirdDense, INIT_LEARNINGRATE, DropoutValue  = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    NodesInThirdDense = int(np.ceil(NodesInThirdDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense(NodesInThirdDense )) # muon and 'other'\n",
    "    model.add(Dense( len(classes) )) # muon and 'other'\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function3(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, NodesInThirdDense, NodesInFourthDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    NodesInThirdDense = int(np.ceil(NodesInThirdDense))\n",
    "    NodesInFourthDense = int(np.ceil(NodesInFourthDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense(NodesInThirdDense )) # muon and 'other'\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInFourthDense )) # muon and 'other'\n",
    "    model.add(Dense( len(classes) ))\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function4(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, NodesInThirdDense, NodesInFourthDense,NodesInFifthDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    NodesInThirdDense = int(np.ceil(NodesInThirdDense))\n",
    "    NodesInFourthDense = int(np.ceil(NodesInFourthDense))\n",
    "    NodesInFifthDense = int(np.ceil(NodesInFifthDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense(NodesInThirdDense )) # muon and 'other'\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInFourthDense )) # muon and 'other'\n",
    "    model.add(Dense(NodesInFifthDense ))\n",
    "    model.add(Dense( len(classes) ))\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_NoBatch(params, X_train=x_train, y_train=y_train, X_score=x_val, y_score=y_val):\n",
    "    \n",
    "    # Optimized parameters\n",
    "    NodesInFirstDense, NodesInSecondDense, INIT_LEARNINGRATE, DropoutValue = params\n",
    "    \n",
    "    # Making sure that the number of nodes are integers\n",
    "    NodesInFirstDense = int(np.ceil(NodesInFirstDense))\n",
    "    NodesInSecondDense = int(np.ceil(NodesInSecondDense))\n",
    "    \n",
    "    # Two parameters not optimized in this case, but can be optimized if needed\n",
    "    BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "    EPOCHS = 3\n",
    "\n",
    "    assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "    #assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NodesInFirstDense, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "    model.add(Dropout(DropoutValue))\n",
    "    model.add(Dense(NodesInSecondDense , activation='relu'))\n",
    "    model.add(Dense( len(classes) ))\n",
    "    model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "        metrics=['accuracy'] \n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting dimensions for optimizeable parameters ###\n",
    "\n",
    "dimensions_4 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]\n",
    "\n",
    "dimensions_5 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInThirdDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]\n",
    "\n",
    "dimensions_6 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInThirdDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInFourthDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]\n",
    "\n",
    "dimensions_7 =[\n",
    "    # NodesInFirstDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInSecondDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInThirdDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInFourthDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # NodesInFifthDense\n",
    "    (4.0, 300.0),\n",
    "    \n",
    "    # LOG_INIT_LEARNINGRATE\n",
    "    (1.0e-4, 1.0e-2),\n",
    "    \n",
    "    # DropoutValue\n",
    "    (0.0, 0.1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random forest regressor optimizer ###\n",
    "\n",
    "bo_rf_41 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_42 = Optimizer(\n",
    "    dimensions=dimensions_5,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_43 = Optimizer(\n",
    "    dimensions=dimensions_6,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_44 = Optimizer(\n",
    "    dimensions=dimensions_7,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=1,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fea4721977c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mOptimalScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     bo_rf_41 = Optimizer(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mdimensions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdimensions_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     base_estimator=RandomForestRegressor(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "ResultsDict = {}\n",
    "for i in range(0,3):\n",
    "    OptimalParams = ([])\n",
    "    OptimalScore = ([])\n",
    "    \n",
    "    bo_rf_41 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_41\n",
    "\n",
    "    for j in range(100):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function1(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    FirstNet = {'Run {0}'.format(i): {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}\n",
    "\n",
    "ResultsDict['First network'] = FirstNet\n",
    "# Second network, three dense layers\n",
    "for i in range(0,3):\n",
    "    \n",
    "    OptimalParams = ([])\n",
    "    OptimalScore = ([])\n",
    "\n",
    "    bo_rf_42 = Optimizer(\n",
    "    dimensions=dimensions_5,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_42\n",
    "\n",
    "    for j in range(150):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function2(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "\n",
    "    OptimalParams = np.append(OptimalParams, best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    SecondNet = {'Run {0}'.format(i): {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}\n",
    "\n",
    "ResultsDict['Second network'] = SecondNet\n",
    "#Third network, four dense layers\n",
    "for i in range(0,3):\n",
    "    \n",
    "    OptimalParams = ([])\n",
    "    OptimalScore = ([])\n",
    "\n",
    "    bo_rf_43 = Optimizer(\n",
    "    dimensions=dimensions_6,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_43\n",
    "\n",
    "    for j in range(180):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function3(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams = np.append(OptimalParams, best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    ThirdNet = {'Run {0}'.format(i): {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}\n",
    "\n",
    "ResultsDict['Third network'] = ThirdNet\n",
    "# fourth network, five dense layers\n",
    "for i in range(0,3):\n",
    "    \n",
    "    OptimalParams = ([])\n",
    "    OptimalScore = ([])\n",
    "\n",
    "    bo_rf_44 = Optimizer(\n",
    "    dimensions=dimensions_7,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6\n",
    "    ),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_44\n",
    "\n",
    "    for j in range(200):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function4(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams = np.append(OptimalParams, best_parameters, axis = 0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    FourthNet = {'Run {0}'.format(i): {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}\n",
    "\n",
    "ResultsDict['Fourth network'] = FourthNet\n",
    "# fifth network, No batch normalization\n",
    "for i in range(0,3):    \n",
    "    \n",
    "    OptimalParams = ([])\n",
    "    OptimalScore = ([])\n",
    "\n",
    "    bo_rf_41 = Optimizer(\n",
    "    dimensions=dimensions_4,\n",
    "    base_estimator=RandomForestRegressor(\n",
    "        n_estimators=100, n_jobs=4, min_variance=1.0e-6),\n",
    "    n_initial_points=10,\n",
    "    acq_func='EI',   \n",
    "    )\n",
    "\n",
    "    bo = bo_rf_41\n",
    "\n",
    "    for j in range(100):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function_NoBatch(x) # Other inputs are automatically set\n",
    "\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "\n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "    OptimalParams = np.append(OptimalParams, best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams, best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    bo.yi[np.argmin(bo.yi)] = 1\n",
    "    \n",
    "    best_result_index = np.argmin(bo.yi)\n",
    "    best_parameters = bo.Xi[best_result_index]\n",
    "    \n",
    "    OptimalParams = np.append(OptimalParams,best_parameters, axis=0)\n",
    "    OptimalScore = np.append(OptimalScore,bo.yi[np.argmin(bo.yi)])\n",
    "    \n",
    "    FifthNet = {'Run {0}'.format(i): {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}\n",
    "\n",
    "ResultsDict['Fifth network'] = FifthNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63773383, 0.64893887, 1.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OptimalScore3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_index = np.argmin(bo.yi)\n",
    "best_parameters = bo.Xi[best_result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5384148328829712"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.yi[np.argmin(bo.yi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.yi[np.argmin(bo.yi)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5414268371482693"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.yi[np.argmin(bo.yi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[173.2773707552993,\n",
       " 14.45812940532587,\n",
       " 0.007589349274725483,\n",
       " 0.008446173333937724]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result_index = np.argmin(bo.yi)\n",
    "best_parameters = bo.Xi[best_result_index]\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53841483])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(OptimalScore1,bo.yi[np.argmin(bo.yi)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5900488114779627"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result_index = np.argmin(bo.yi)\n",
    "best_parameters = bo.Xi[best_result_index]\n",
    "\n",
    "best_result_index\n",
    "best_parameters\n",
    "bo.yi[np.argmin(bo.yi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gaussian optimizer ###\n",
    "\n",
    "bo_gp_1 = Optimizer(\n",
    "    ### telling optimizer boundaries for each parameter\n",
    "    dimensions=dimensions_1,\n",
    "    \n",
    "    ### setting regressor\n",
    "    base_estimator=GaussianProcessRegressor(\n",
    "        kernel=RBF(length_scale_bounds=[1.0e-6, 1.0e+6]) + \\\n",
    "            WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2]),\n",
    "    ),\n",
    "    n_initial_points=2,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Actual optimization process ###\n",
    "def Optimization(bo,target_function):\n",
    "    Parameter_List = ([])\n",
    "    for i in range(1):\n",
    "        x = bo.ask()\n",
    "        print(x)\n",
    "        f = target_function(x) # Other inputs are automatically set\n",
    "\n",
    "        Parameter_List = np.append(Parameter_List, x)\n",
    "        bo.tell(x, f)\n",
    "\n",
    "        plot_convergence(bo)\n",
    "        \n",
    "        best_result_index = np.argmin(bo.yi)\n",
    "        best_parameters = bo.Xi[best_result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:\n",
      " Nodes in first dense layer= 235 \n",
      " Nodes in second dense layer= 147 \n",
      " learning rate= 0.02177968898175681 \n",
      " Dropout value= 0.005580370632707155\n"
     ]
    }
   ],
   "source": [
    "print_best(bo_rf_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 271)               17886     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 271)               1084      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 271)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 280)               76160     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 562       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 95,692\n",
      "Trainable params: 95,150\n",
      "Non-trainable params: 542\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 14s 321us/step - loss: 0.2786 - acc: 0.9236 - val_loss: 0.2719 - val_acc: 0.9250\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 265us/step - loss: 0.2433 - acc: 0.9250 - val_loss: 0.2521 - val_acc: 0.9247\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 249us/step - loss: 0.2363 - acc: 0.9251 - val_loss: 0.2567 - val_acc: 0.9266\n",
      "0.3481612531447594\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 260us/step - loss: 0.2347 - acc: 0.9254 - val_loss: 0.2492 - val_acc: 0.9261\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 266us/step - loss: 0.2334 - acc: 0.9253 - val_loss: 0.2366 - val_acc: 0.9262\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 243us/step - loss: 0.2326 - acc: 0.9258 - val_loss: 0.2347 - val_acc: 0.9266\n",
      "0.3777592072609885\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 246us/step - loss: 0.2322 - acc: 0.9259 - val_loss: 0.2292 - val_acc: 0.9276\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 278us/step - loss: 0.2303 - acc: 0.9265 - val_loss: 0.2333 - val_acc: 0.9264\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 245us/step - loss: 0.2311 - acc: 0.9263 - val_loss: 0.2338 - val_acc: 0.9273\n",
      "0.3755042846797935\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 261us/step - loss: 0.2310 - acc: 0.9257 - val_loss: 0.2362 - val_acc: 0.9255\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 12s 268us/step - loss: 0.2297 - acc: 0.9270 - val_loss: 0.2340 - val_acc: 0.9248\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 11s 248us/step - loss: 0.2309 - acc: 0.9260 - val_loss: 0.2338 - val_acc: 0.9249\n",
      "0.4127228370619375\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 11s 262us/step - loss: 0.2295 - acc: 0.9256 - val_loss: 0.2429 - val_acc: 0.9262\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 11s 253us/step - loss: 0.2300 - acc: 0.9252 - val_loss: 0.2349 - val_acc: 0.9266\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 12s 268us/step - loss: 0.2312 - acc: 0.9258 - val_loss: 0.2394 - val_acc: 0.9265\n",
      "0.3827155505030586\n"
     ]
    }
   ],
   "source": [
    "### Testing the found parameters to see if the results are reproduceable ###\n",
    "\n",
    "INIT_LEARNINGRATE = 0.02532767329545395  \n",
    "BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "EPOCHS = 3\n",
    "\n",
    "assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "#assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(271, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.0009682674694188368))\n",
    "model.add(Dense(280, activation='relu'))\n",
    "model.add(Dense( len(classes) )) # muon and 'other'\n",
    "model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "    metrics=['accuracy'] \n",
    "    )\n",
    "\n",
    "# Running test for 5 times in this case\n",
    "for i in range(0,5):\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = (x_val, y_val),\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "\n",
    "    #model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "    # score\n",
    "\n",
    "    validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "    result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "    print(result)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               6600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 12,152\n",
      "Trainable params: 11,952\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 43565 samples, validate on 10892 samples\n",
      "Epoch 1/3\n",
      "43565/43565 [==============================] - 8s 176us/step - loss: 0.2471 - acc: 0.9252 - val_loss: 0.2437 - val_acc: 0.9234\n",
      "Epoch 2/3\n",
      "43565/43565 [==============================] - 6s 143us/step - loss: 0.2357 - acc: 0.9267 - val_loss: 0.2456 - val_acc: 0.9224\n",
      "Epoch 3/3\n",
      "43565/43565 [==============================] - 6s 141us/step - loss: 0.2323 - acc: 0.9263 - val_loss: 0.2408 - val_acc: 0.9236\n",
      "0.3487180061696864\n"
     ]
    }
   ],
   "source": [
    "#### Original keras model for reference ####\n",
    "\n",
    "INIT_LEARNINGRATE = 5e-3\n",
    "BATCH_SIZE = 16  # should be a factor of len(x_train) and len(x_val) etc.\n",
    "EPOCHS = 3\n",
    "\n",
    "assert len(y_train) == len(x_train), \"x_train and y_train not same length!\"\n",
    "#assert len(y_train) % BATCH_SIZE == 0, \"batch size should be multiple of training size,{0}/{1}\".format(len(y_train),BATCH_SIZE)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=( len( utils.SIMPLE_FEATURE_COLUMNS ), ))) #length = input vars\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50 , activation='relu'))\n",
    "model.add(Dense( len(classes) )) # muon and 'other'\n",
    "model.add(Activation(\"softmax\")) # output probabilities\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.adamax(lr=INIT_LEARNINGRATE),\n",
    "    metrics=['accuracy'] \n",
    "    )\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (x_val, y_val),\n",
    "    shuffle = True\n",
    "    )\n",
    "\n",
    "#model.save_model(\"keras_basic_model.xgb\")\n",
    "\n",
    "\n",
    "# score\n",
    "\n",
    "validation_predictions = model.predict_proba(val_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values)[:, 1]\n",
    "result = scoring.rejection90(val_part.label.values, validation_predictions, sample_weight=val_part.weight.values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    \n",
    "    for j in range(0,5):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nproc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (65,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"c:\\miniconda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 952, in fit\n    batch_size=batch_size)\n  File \"c:\\miniconda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 751, in _standardize_user_data\n    exception_prefix='input')\n  File \"c:\\miniconda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 138, in standardize_input_data\n    str(data_shape))\nValueError: Error when checking input: expected dense_1_input to have shape (65,) but got array with shape (2,)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f031f348a31a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         '''\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda\\envs\\py36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (65,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "r = pool.map(model.fit, [x_train, y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "DataDict = {}\n",
    "DataDict = {'FirstNetwork' : {'run {0}'.format(i) : 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataDict['FirstNetwork']['run 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-eb9632e05b5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mOptimalParams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mOptimalScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mResultsDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "OptimalParams = np.array([9,3,2,3])\n",
    "OptimalScore = np.array([8,3,3,3,3,2,2,2])\n",
    "ResultsDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'a'\n",
    "# SecondNet = {'Run {0}'.format(i): {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}\n",
    "ResultsDict = {'Second network' : { i : {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}}\n",
    "\n",
    "i = 'b'\n",
    "\n",
    "OptimalParams = np.array([1,3,2,3])\n",
    "OptimalScore = np.array([1,3,3,3])\n",
    "\n",
    "\n",
    "ResultsDict['Second network'][i] = {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}\n",
    "#wordFreqDic.update(test = 'value' )\n",
    "#ResultsDict['Second network'] = SecondNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Second network': {'a': {'Optimal Parameters': array([1, 3, 2, 3]),\n",
       "   'Optimal Score': array([1, 3, 3, 3])},\n",
       "  'b': {'Optimal Parameters': array([1, 3, 2, 3]),\n",
       "   'Optimal Score': array([1, 3, 3, 3])}}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResultsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "SecondNet = {'Run {0}'.format(i): {'Optimal Parameters' : OptimalParams, 'Optimal Score' : OptimalScore}}\n",
    "\n",
    "ResultsDict['Second network'] = SecondNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Run 1': {'Optimal Parameters': array([1, 3, 2, 3]),\n",
       "  'Optimal Score': array([1, 3, 3, 3])}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResultsDict = {'Second network' : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "for i in ['a', 'b', 'c']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
